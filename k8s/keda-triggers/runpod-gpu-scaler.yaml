apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: runpod-gpu-scaler
  namespace: media
spec:
  minReplicaCount: 0
  maxReplicaCount: 20
  scaleTargetRef:
    name: ome-transcoder
  
  triggers:
  # Kafka queue depth trigger
  - type: kafka
    metadata:
      bootstrapServers: kafka-client.data.svc.cluster.local:9092
      consumerGroup: ome-transcoder-consumer
      topic: video-transcode
      lagThreshold: '5'  # Trigger at 5 jobs queued
      offsetResetPolicy: earliest
  
  # Latency SLO trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.observability.svc.cluster.local:9090
      metricName: transcoding_latency_p95
      threshold: '30000'  # 30 seconds
      query: |
        histogram_quantile(0.95, 
          sum(rate(ome_transcoding_duration_seconds_bucket[5m])) 
          by (le)
        ) * 1000
  
  # Custom RunPod trigger (via HTTP scaler)
  - type: external
    metadata:
      scalerAddress: runpod-autoscaler.platform.svc.cluster.local:8080
      queueName: gpu-queue
      queueLength: '5'
  
  fallback:
    failureThreshold: 3
    replicas: 1

---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: runpod-local-gpu-checkpoint
  namespace: media
spec:
  minReplicaCount: 0
  maxReplicaCount: 2
  scaleTargetRef:
    name: local-gpu-checkpoint
  
  triggers:
  # Scale local GPU only for ultra-low-latency jobs
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.observability.svc.cluster.local:9090
      metricName: low_latency_job_queue
      threshold: '1'
      query: |
        sum(job_queue_size{latency_slo="ultra_low"})
  
  fallback:
    failureThreshold: 5
    replicas: 0

